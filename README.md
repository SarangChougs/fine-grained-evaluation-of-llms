
# Fine-Grained Evaluation of LLMs for Text Generation and Open-Ended QA

This project investigates the **fine-grained evaluation of open-source Large Language Models (LLMs)** with a focus on **text generation** and **open-ended question answering** tasks. Inspired by frameworks like **FLASK** and tools like **Prometheus**, this study proposes a **custom evaluation rubric** designed to align more closely with human judgment.

---

## ğŸ§  Abstract

Evaluating LLMs remains a challenging task due to the complex and subjective nature of language. This study explores whether a **FLASK-like fine-grained evaluation framework** can offer a more nuanced and human-aligned assessment of LLM outputs. Ten LLaMA-based open-source models were evaluated on:

- **Standard metrics**: BLEU, ROUGE, BERTScore
- **Custom fine-grained criteria**: Tailored specifically for text generation and open-ended QA

Correlation analysis revealed a **strong alignment** between our custom metric and BERTScore/ROUGE-L, suggesting its effectiveness.

---

## ğŸ“š Keywords

`LLM` Â· `text-generation` Â· `open-ended question answering` Â· `fine-grained evaluation` Â· `Prometheus` Â· `FLASK`

---

## ğŸ” Project Structure

```

â”œâ”€â”€ data/                # Evaluation datasets (custom instructions)
â”œâ”€â”€ models/              # Config files and model download instructions
â”œâ”€â”€ evaluations/
â”‚   â”œâ”€â”€ standard/        # BLEU, ROUGE, BERTScore results
â”‚   â””â”€â”€ custom/          # Prometheus-based fine-grained results
â”œâ”€â”€ prompts/             # Standardized prompt formats per model
â”œâ”€â”€ analysis/            # Correlation analysis and visualizations
â”œâ”€â”€ report/              # Final report and appendix
â””â”€â”€ README.md

```

---

## ğŸ§ª Evaluation Overview

### âœ… Standard Metrics
- **BLEU**
- **ROUGE-L**
- **BERTScore (Precision, Recall, F1)**

### ğŸŒŸ Custom Fine-Grained Criteria (via Prometheus)
Evaluated across dimensions like:
- Relevance
- Clarity & Coherence
- Completeness
- Accuracy
- Logical Soundness
- Creativity
- Engagement
- Empathy
- Efficiency

> Each response is scored from **1 (poor)** to **5 (excellent)** based on rubrics derived from human communication norms.

---

## ğŸ“Š Key Findings

| Metric      | Correlation with Prometheus |
|-------------|-----------------------------|
| BLEU        | Weak or Negative (-0.76)    |
| BERTScore   | Strong Positive (~0.81)     |
| ROUGE-L     | Strong Positive (~0.81)     |

- Models like **WizardLM-13B-V1.1** scored highest overall.
- Custom metrics **align more with semantic and contextual quality** than lexical similarity.

---

## ğŸ”§ Tools & Models

- **LLMs**: 10 open-source LLaMA-based models (â‰¤13B parameters)
- **Evaluation Engine**: [Prometheus](https://github.com/kaistAI/Prometheus)
- **Dataset**: Custom-built, inspired by Alpaca, Ultrachat, No Robots
- **Hardware**: NVIDIA A800 40GB GPU

---

## ğŸ§  Future Work

- Benchmarking against **FLASK** and **Human Evaluation**
- Expand to more instruction types (summarization, critique, etc.)
- Use **state-of-the-art closed models** like ChatGPT for comparison

---

## ğŸ“„ Citation

If you use this work or build upon it, please cite:
```

@misc{finegrained-llm-eval,
title={Fine-Grained Evaluation of LLMs},
author={Your Name},
year={2025},
url={[https://github.com/yourusername/llm-finegrained-eval}](https://github.com/yourusername/llm-finegrained-eval})
}

```

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“¬ Contact

For questions or collaboration, feel free to reach out at `sarangchouguley284@example.com`.

```

---
