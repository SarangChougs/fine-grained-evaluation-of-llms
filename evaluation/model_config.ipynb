{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e955889-89b5-485b-ad78-14073db29b19",
   "metadata": {},
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f466039-0250-4d94-8677-770944b94d49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Using cached huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Using cached safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Using cached huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: safetensors, regex, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 huggingface-hub-0.20.2 regex-2023.12.25 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5280e15d-beb1-4aad-9e66-1fe4cb75d74b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Collecting autoawq\n",
      "  Downloading autoawq-0.1.8-cp310-cp310-manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting torch>=2.0.1 (from autoawq)\n",
      "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: transformers>=4.35.0 in /opt/conda/lib/python3.10/site-packages (from autoawq) (4.36.2)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from autoawq) (0.15.0)\n",
      "Collecting accelerate (from autoawq)\n",
      "  Using cached accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sentencepiece (from autoawq)\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting lm-eval (from autoawq)\n",
      "  Using cached lm_eval-0.4.0-py3-none-any.whl\n",
      "Collecting texttable (from autoawq)\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting toml (from autoawq)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting attributedict (from autoawq)\n",
      "  Downloading attributedict-0.3.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from autoawq) (4.24.4)\n",
      "Collecting torchvision (from autoawq)\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting tabulate (from autoawq)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.12.1->autoawq) (0.20.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.1->autoawq) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.1->autoawq) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.1->autoawq) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.1->autoawq) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.1->autoawq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.1->autoawq) (2023.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==2.1.0 (from torch>=2.0.1->autoawq)\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.1->autoawq)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (4.66.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->autoawq) (5.9.5)\n",
      "Collecting rootpath>=0.1.0 (from attributedict->autoawq)\n",
      "  Downloading rootpath-0.1.1-py3-none-any.whl (15 kB)\n",
      "Collecting inspecta>=0.1.0 (from attributedict->autoawq)\n",
      "  Downloading inspecta-0.1.3-py3-none-any.whl (9.2 kB)\n",
      "Collecting colour-runner>=0.0.5 (from attributedict->autoawq)\n",
      "  Downloading colour_runner-0.1.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting deepdiff>=3.3.0 (from attributedict->autoawq)\n",
      "  Downloading deepdiff-6.7.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tox>=3.0.0 (from attributedict->autoawq)\n",
      "  Downloading tox-4.12.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting coverage>=4.5.2 (from attributedict->autoawq)\n",
      "  Downloading coverage-7.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting codecov>=2.0.15 (from attributedict->autoawq)\n",
      "  Downloading codecov-2.1.13-py2.py3-none-any.whl (16 kB)\n",
      "Collecting evaluate (from lm-eval->autoawq)\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting datasets>=2.0.0 (from lm-eval->autoawq)\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jsonlines (from lm-eval->autoawq)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numexpr in /opt/conda/lib/python3.10/site-packages (from lm-eval->autoawq) (2.8.7)\n",
      "Collecting peft>=0.2.0 (from lm-eval->autoawq)\n",
      "  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pybind11>=2.6.2 (from lm-eval->autoawq)\n",
      "  Downloading pybind11-2.11.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting pytablewriter (from lm-eval->autoawq)\n",
      "  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting rouge-score>=0.0.4 (from lm-eval->autoawq)\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Collecting sacrebleu>=1.5.0 (from lm-eval->autoawq)\n",
      "  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from lm-eval->autoawq) (1.3.2)\n",
      "Collecting sqlitedict (from lm-eval->autoawq)\n",
      "  Using cached sqlitedict-2.1.0-py3-none-any.whl\n",
      "Collecting tqdm-multiprocess (from lm-eval->autoawq)\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from lm-eval->autoawq) (0.22.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->autoawq) (10.1.0)\n",
      "Collecting blessings (from colour-runner>=0.0.5->attributedict->autoawq)\n",
      "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.10/site-packages (from colour-runner>=0.0.5->attributedict->autoawq) (2.16.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->lm-eval->autoawq) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->lm-eval->autoawq) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->lm-eval->autoawq) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->lm-eval->autoawq) (2.1.3)\n",
      "Collecting xxhash (from datasets>=2.0.0->lm-eval->autoawq)\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets>=2.0.0->lm-eval->autoawq)\n",
      "  Using cached multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->lm-eval->autoawq) (3.8.6)\n",
      "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff>=3.3.0->attributedict->autoawq)\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Collecting responses<0.19 (from evaluate->lm-eval->autoawq)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from inspecta>=0.1.0->attributedict->autoawq) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from inspecta>=0.1.0->attributedict->autoawq)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.35.0->autoawq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.35.0->autoawq) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.35.0->autoawq) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.35.0->autoawq) (2023.7.22)\n",
      "Collecting coloredlogs>=10.0 (from rootpath>=0.1.0->attributedict->autoawq)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm-eval->autoawq) (2.0.0)\n",
      "Collecting nltk (from rouge-score>=0.0.4->lm-eval->autoawq)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu>=1.5.0->lm-eval->autoawq)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm-eval->autoawq) (0.4.6)\n",
      "Collecting lxml (from sacrebleu>=1.5.0->lm-eval->autoawq)\n",
      "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm-eval->autoawq) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm-eval->autoawq) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm-eval->autoawq) (3.2.0)\n",
      "Requirement already satisfied: cachetools>=5.3.2 in /opt/conda/lib/python3.10/site-packages (from tox>=3.0.0->attributedict->autoawq) (5.3.2)\n",
      "Collecting chardet>=5.2 (from tox>=3.0.0->attributedict->autoawq)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting platformdirs>=4.1 (from tox>=3.0.0->attributedict->autoawq)\n",
      "  Downloading platformdirs-4.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pluggy>=1.3 in /opt/conda/lib/python3.10/site-packages (from tox>=3.0.0->attributedict->autoawq) (1.3.0)\n",
      "Collecting pyproject-api>=1.6.1 (from tox>=3.0.0->attributedict->autoawq)\n",
      "  Downloading pyproject_api-1.6.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from tox>=3.0.0->attributedict->autoawq) (2.0.1)\n",
      "Collecting virtualenv>=20.25 (from tox>=3.0.0->attributedict->autoawq)\n",
      "  Downloading virtualenv-20.25.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.1->autoawq) (2.1.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines->lm-eval->autoawq) (23.1.0)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm-eval->autoawq) (68.2.2)\n",
      "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm-eval->autoawq)\n",
      "  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval->autoawq)\n",
      "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval->autoawq)\n",
      "  Downloading pathvalidate-3.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval->autoawq)\n",
      "  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval->autoawq)\n",
      "  Downloading tcolorpy-0.1.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->autoawq)\n",
      "  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0.1->autoawq) (1.3.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs>=10.0->rootpath>=0.1.0->attributedict->autoawq)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->autoawq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.9 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->autoawq) (2023.3.post1)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.25->tox>=3.0.0->attributedict->autoawq)\n",
      "  Downloading distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score>=0.0.4->lm-eval->autoawq) (8.1.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.0.0->lm-eval->autoawq) (2023.3)\n",
      "Downloading autoawq-0.1.8-cp310-cp310-manylinux2014_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Using cached accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coverage-7.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "Downloading deepdiff-6.7.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tox-4.12.1-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
      "Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
      "Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
      "Downloading platformdirs-4.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading pyproject_api-1.6.1-py3-none-any.whl (12 kB)\n",
      "Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
      "Downloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
      "Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: texttable, sqlitedict, sentencepiece, distlib, xxhash, triton, tqdm-multiprocess, toml, termcolor, tcolorpy, tabulate, pyproject-api, pybind11, portalocker, platformdirs, pathvalidate, ordered-set, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nltk, multiprocess, lxml, jsonlines, humanfriendly, coverage, chardet, blessings, virtualenv, sacrebleu, rouge-score, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mbstrdecoder, deepdiff, colour-runner, coloredlogs, codecov, typepy, tox, nvidia-cusolver-cu12, torch, rootpath, datasets, torchvision, inspecta, evaluate, DataProperty, accelerate, tabledata, peft, attributedict, pytablewriter, lm-eval, autoawq\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.0.0\n",
      "    Uninstalling platformdirs-4.0.0:\n",
      "      Successfully uninstalled platformdirs-4.0.0\n",
      "Successfully installed DataProperty-1.0.1 accelerate-0.26.1 attributedict-0.3.0 autoawq-0.1.8 blessings-1.7 chardet-5.2.0 codecov-2.1.13 coloredlogs-15.0.1 colour-runner-0.1.1 coverage-7.4.0 datasets-2.16.1 deepdiff-6.7.1 distlib-0.3.8 evaluate-0.4.1 humanfriendly-10.0 inspecta-0.1.3 jsonlines-4.0.0 lm-eval-0.4.0 lxml-5.1.0 mbstrdecoder-1.1.3 multiprocess-0.70.15 nltk-3.8.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 ordered-set-4.1.0 pathvalidate-3.2.0 peft-0.7.1 platformdirs-4.1.0 portalocker-2.8.2 pybind11-2.11.1 pyproject-api-1.6.1 pytablewriter-1.2.0 responses-0.18.0 rootpath-0.1.1 rouge-score-0.1.2 sacrebleu-2.4.0 sentencepiece-0.1.99 sqlitedict-2.1.0 tabledata-1.3.3 tabulate-0.9.0 tcolorpy-0.1.4 termcolor-2.4.0 texttable-1.7.0 toml-0.10.2 torch-2.1.2 torchvision-0.16.2 tox-4.12.1 tqdm-multiprocess-0.0.11 triton-2.1.0 typepy-1.3.2 virtualenv-20.25.0 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install autoawq #Loading an AWQ quantized model requires auto-awq library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc534b3-f720-4b40-9647-969e19284a4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu117/\n",
      "Collecting auto-gptq\n",
      "  Downloading auto_gptq-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: accelerate>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.26.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.16.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.1.99)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.26.0)\n",
      "Collecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Collecting gekko (from auto-gptq)\n",
      "  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.4.1)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.36.2)\n",
      "Requirement already satisfied: peft>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.7.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (0.20.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.15.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.8.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Downloading auto_gptq-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rouge, gekko, auto-gptq\n",
      "Successfully installed auto-gptq-0.6.0 gekko-1.0.6 rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/ # Libraries for GPTQ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fd35d3-a1ed-417f-afb9-855be6fb238e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Collecting optimum\n",
      "  Using cached optimum-1.16.2-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (4.36.2)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.1.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (23.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.20.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.16.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (0.4.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (4.24.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.8.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Using cached optimum-1.16.2-py3-none-any.whl (402 kB)\n",
      "Installing collected packages: optimum\n",
      "Successfully installed optimum-1.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359f1a71-93a6-492d-bf06-af407f7dc385",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.20.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.5)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.1.2)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.1.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.36.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert-score) (1.26.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.66.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.8.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.20.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.44.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n",
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (2.0.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install nltk\n",
    "!pip install bert-score\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c683b6-e3b1-4501-84ca-bc94ab3d9eaf",
   "metadata": {},
   "source": [
    "- do_sample - if set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary with various strategy-specific adjustments.\n",
    "- return_full_text (bool, optional, defaults to True) — If set to False only added text is returned, otherwise the full text is returned. Only meaningful if return_text is set to True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7307b-7af4-4bbe-978c-8a8332872c28",
   "metadata": {},
   "source": [
    "## Microsoft/Orca code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10335a8-f985-4c5d-a191-685869cd7ae5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "else:\n",
    "    torch.set_default_device(\"cpu\")\n",
    "    \n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Orca-2-7b\", device_map='auto')\n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/27132\n",
    "# please use the slow tokenizer since fast and slow tokenizer produces different tokens\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"microsoft/Orca-2-7b\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "system_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\n",
    "user_message = \"How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful?\"\n",
    "\n",
    "prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output_ids = model.generate(inputs[\"input_ids\"],)\n",
    "answer = tokenizer.batch_decode(output_ids)[0]\n",
    "\n",
    "print(answer)\n",
    "\n",
    "# This example continues showing how to add a second turn message by the user to the conversation\n",
    "second_turn_user_message = \"Give me a list of the key points of your first answer.\"\n",
    "\n",
    "# we set add_special_tokens=False because we dont want to automatically add a bos_token between messages\n",
    "second_turn_message_in_markup = f\"\\n<|im_start|>user\\n{second_turn_user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "second_turn_tokens = tokenizer(second_turn_message_in_markup, return_tensors='pt', add_special_tokens=False)\n",
    "second_turn_input = torch.cat([output_ids, second_turn_tokens['input_ids']], dim=1)\n",
    "\n",
    "output_ids_2 = model.generate(second_turn_input,)\n",
    "second_turn_answer = tokenizer.batch_decode(output_ids_2)[0]\n",
    "\n",
    "print(second_turn_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6e97c-5f58-4bc6-93c3-1b12c7f68671",
   "metadata": {},
   "source": [
    "## Yi-34b-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7914f59-c6ee-49cd-97db-bca467af43d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = '01-ai/Yi-34b-Chat'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n",
    "# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype='auto'\n",
    ").eval()\n",
    "\n",
    "# Prompt content: \"hi\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"hi\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = model.generate(input_ids.to('cuda'))\n",
    "response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Model response: \"Hello! How can I assist you today?\"\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8019c-ad3e-43f7-8e6d-537153d1463c",
   "metadata": {},
   "source": [
    "## WizardLM 13B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a2968-f9d1-4093-8b1f-4c36047e561c",
   "metadata": {},
   "source": [
    "This code is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4067e6dc-a795-40e8-9c41-1e1b5fec222e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65db7158cc2d437da18a634a020e238f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{\"id\":\"tg1\",\"input\":\"I need an 80s sci-fi fantasy character introduction for my newest novel. The main character’s name is Drake, and the year is 3098.\",\"target\":\"The flashing neon lights bounced off Drake’s face as he stepped out of the shadows. As a rogue bounty hunter, there was nothing Drake feared, and the scar covering his left eye proved this even further. \\n\\nHe had otherworldly energy about him, his electric undercut framing his face. His lone eye was like a star, burning wildly in his journey to find out the truth regarding his family. As he walked through the streets, heads turned in awe—this hero was synonymous with hope for the people and the city, as 3098 was the universe’s most dangerous year.\\n\\nIn Drake’s holster was a next-generation laser pistol capable of exterminating dangerous cyborgs in just one good shot. It was clear Drake was fighting on the side of humanity, clinging to hope that his own agenda would someday be fulfilled.\",\"type\":\"text-generation\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "#import fire\n",
    "import torch\n",
    "# from peft import PeftModel\n",
    "import transformers\n",
    "# import gradio as gr\n",
    "import json\n",
    "\n",
    "assert (\n",
    "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
    "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def main(\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"WizardLM/WizardLM-13B-V1.1\",\n",
    "    input_data_path = \"custom_data.jsonl\",   \n",
    "    output_data_path = \"WizardLM_testset_output.jsonl\",\n",
    "    \n",
    "):\n",
    "    assert base_model, (\n",
    "        \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "    if device == \"cuda\":\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            load_in_8bit=load_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "    # unwind broken decapoda-research config\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "\n",
    "    def inference(\n",
    "            batch_data,\n",
    "            input=None,\n",
    "            temperature=1,\n",
    "            top_p=0.95,\n",
    "            top_k=40,\n",
    "            num_beams=1,\n",
    "            max_new_tokens=2048,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        prompts = f\"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {batch_data} ASSISTANT:\"\"\"\n",
    "        \n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            #top_p=top_p,\n",
    "            #top_k=top_k,\n",
    "            #num_beams=num_beams,\n",
    "            #do_sample=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences       \n",
    "        output = tokenizer.batch_decode(s, skip_special_tokens=True)\n",
    "        output = output[0].split(\"ASSISTANT:\")[1].strip()\n",
    "       \n",
    "        return output\n",
    "\n",
    "    \n",
    "    input_data = open(input_data_path, mode='r', encoding='utf-8')\n",
    "    output_data = open(output_data_path, mode='w', encoding='utf-8')\n",
    "   \n",
    "    for num, line in enumerate(input_data.readlines()):\n",
    "        print(num)\n",
    "        print(line)\n",
    "        \n",
    "        one_data = json.loads(line)\n",
    "        id = one_data[\"id\"]\n",
    "        Category = one_data[\"type\"]\n",
    "        instruction = one_data[\"input\"]\n",
    "\n",
    "        final_output = inference(instruction)\n",
    "           \n",
    "        new_data = {\n",
    "                    \"id\": id,                   \n",
    "                    \"instruction\": instruction,\n",
    "                    \"wizardlm-13b\": final_output\n",
    "                }\n",
    "        output_data.write(json.dumps(new_data) + '\\n')\n",
    "            \n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #fire.Fire(main)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60ec97-cf89-4be6-910f-104e17a68f0b",
   "metadata": {},
   "source": [
    "## Wizard LM 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdacc8c-79f6-4647-b8bf-04dc7df51968",
   "metadata": {},
   "source": [
    "Code not working. Generating gibbirish model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb32a8ff-9f97-47b5-ae5f-18966666a65e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0aa56382de4477d95bf84f87cb579da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "#import fire\n",
    "import torch\n",
    "import transformers\n",
    "import json\n",
    "\n",
    "assert (\n",
    "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
    "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def main(\n",
    "    count,\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"WizardLM/WizardLM-7B-V1.0\",\n",
    "    input_data_path = \"custom_data.jsonl\",\n",
    "    output_data_path = \"WizardLM_7b_Output.jsonl\",\n",
    "):\n",
    "    assert base_model, (\n",
    "        \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "    if device == \"cuda\":\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            load_in_8bit=load_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    input_data = open(input_data_path, mode='r', encoding='utf-8')\n",
    "    output_data = open(output_data_path, mode='w', encoding='utf-8')\n",
    "\n",
    "    for num, line in enumerate(input_data.readlines()):\n",
    "        if num < count:\n",
    "            print(num)\n",
    "            one_data = json.loads(line)\n",
    "            id = one_data[\"id\"]\n",
    "            instruction = one_data[\"input\"]\n",
    "            _output = evaluate(instruction, tokenizer, model)\n",
    "            final_output = _output[0].split(\"### Response:\")[1].strip()\n",
    "            new_data = {\n",
    "                \"id\": id,\n",
    "                \"instruction\": instruction,\n",
    "                \"wizardlm\": final_output\n",
    "            }\n",
    "            output_data.write(json.dumps(new_data) + '\\n')\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "        batch_data,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        input=None,\n",
    "        temperature=1,\n",
    "        top_p=0.9,\n",
    "        top_k=40,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=2048,\n",
    "        **kwargs,\n",
    "):\n",
    "    prompts = generate_prompt(batch_data, input)\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        #temperature=temperature,\n",
    "        #top_p=top_p,\n",
    "        #top_k=top_k,\n",
    "        #num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences\n",
    "    output = tokenizer.batch_decode(s, skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "\n",
    "def generate_prompt(instruction, input=None):\n",
    "    return f\"\"\"{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #fire.Fire(main)\n",
    "main(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d435648-699a-404b-90ad-b4988f77bf9f",
   "metadata": {},
   "source": [
    "## Wizard LM Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307390a8-ea36-4563-8c2c-d999c278ca1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--base_model\", type=str)  # model path\n",
    "    parser.add_argument(\"--n_gpus\", type=int, default=1)  # n_gpu\n",
    "    return parser.parse_args()\n",
    "\n",
    "def predict(message, history, system_prompt, temperature, max_tokens):\n",
    "    instruction = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \"\n",
    "    for human, assistant in history:\n",
    "        instruction += 'USER: '+ human + ' ASSISTANT: '+ assistant + '</s>'\n",
    "    instruction += 'USER: '+ message + ' ASSISTANT:'\n",
    "    problem = [instruction]\n",
    "    stop_tokens = [\"USER:\", \"USER\", \"ASSISTANT:\", \"ASSISTANT\"]\n",
    "    sampling_params = SamplingParams(temperature=temperature, top_p=1, max_tokens=max_tokens, stop=stop_tokens)\n",
    "    completions = llm.generate(problem, sampling_params)\n",
    "    for output in completions:\n",
    "        prompt = output.prompt\n",
    "        print('==========================question=============================')\n",
    "        print(prompt)\n",
    "        generated_text = output.outputs[0].text\n",
    "        print('===========================answer=============================')\n",
    "        print(generated_text)\n",
    "        for idx in range(len(generated_text)):\n",
    "                yield generated_text[:idx+1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    llm = LLM(model=args.base_model, tensor_parallel_size=args.n_gpus)\n",
    "    gr.ChatInterface(\n",
    "        predict,\n",
    "        title=\"LLM playground - WizardLM-13B-V1.2\",\n",
    "        description=\"This is a LLM playground for WizardLM-13B-V1.2, github: https://github.com/nlpxucan/WizardLM, huggingface: https://huggingface.co/WizardLM\",\n",
    "        theme=\"soft\",\n",
    "        chatbot=gr.Chatbot(height=1400, label=\"Chat History\",),\n",
    "        textbox=gr.Textbox(placeholder=\"input\", container=False, scale=7),\n",
    "        retry_btn=None,\n",
    "        undo_btn=\"Delete Previous\",\n",
    "        clear_btn=\"Clear\",\n",
    "        additional_inputs=[\n",
    "            gr.Textbox(\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", label=\"System Prompt\"),\n",
    "            gr.Slider(0, 1, 0.9, label=\"Temperature\"),\n",
    "            gr.Slider(100, 2048, 1024, label=\"Max Tokens\"),\n",
    "        ],\n",
    "        additional_inputs_accordion_name=\"Parameters\",\n",
    "    ).queue().launch(share=False, server_port=7870)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670db7bb-c1e3-4381-a3c7-ac52b19cc0fd",
   "metadata": {},
   "source": [
    "## Llava 13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3d259-dfee-43ea-99f0-4c975be7d5a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"TheBloke/llava-v1.5-13B-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''{prompt}\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))\n",
    "\n",
    "\"\"\"\n",
    "# Inference should be possible with transformers pipeline as well in future\n",
    "# But currently this is not yet supported by AutoAWQ (correct as of September 25th 2023)\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e9f33-281a-4972-a5a4-66c5b5016569",
   "metadata": {},
   "source": [
    "## Wizardlm 7B V1.0 Uncensored - GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6031c74-827f-46fe-806b-b6c5838dc04e",
   "metadata": {},
   "source": [
    "working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f61f3e9b-abcd-4e13-9a54-80d16ce7374d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at TheBloke/WizardLM-7B-V1.0-Uncensored-GPTQ and are newly initialized: ['model.layers.21.mlp.gate_proj.g_idx', 'model.layers.6.mlp.up_proj.g_idx', 'model.layers.19.mlp.gate_proj.g_idx', 'model.layers.6.mlp.gate_proj.g_idx', 'model.layers.4.self_attn.k_proj.g_idx', 'model.layers.3.mlp.gate_proj.g_idx', 'model.layers.22.mlp.down_proj.g_idx', 'model.layers.12.self_attn.v_proj.g_idx', 'model.layers.26.mlp.up_proj.g_idx', 'model.layers.19.self_attn.o_proj.g_idx', 'model.layers.4.self_attn.q_proj.g_idx', 'model.layers.17.self_attn.v_proj.g_idx', 'model.layers.22.mlp.up_proj.g_idx', 'model.layers.11.mlp.up_proj.g_idx', 'model.layers.7.mlp.down_proj.g_idx', 'model.layers.13.self_attn.k_proj.g_idx', 'model.layers.11.self_attn.q_proj.g_idx', 'model.layers.15.self_attn.k_proj.g_idx', 'model.layers.11.self_attn.v_proj.g_idx', 'model.layers.8.mlp.gate_proj.g_idx', 'model.layers.28.self_attn.q_proj.g_idx', 'model.layers.6.self_attn.o_proj.g_idx', 'model.layers.19.self_attn.v_proj.g_idx', 'model.layers.27.self_attn.v_proj.g_idx', 'model.layers.27.mlp.up_proj.g_idx', 'model.layers.18.self_attn.q_proj.g_idx', 'model.layers.25.mlp.gate_proj.g_idx', 'model.layers.5.mlp.down_proj.g_idx', 'model.layers.0.self_attn.o_proj.g_idx', 'model.layers.0.mlp.up_proj.g_idx', 'model.layers.9.self_attn.v_proj.g_idx', 'model.layers.1.mlp.gate_proj.g_idx', 'model.layers.29.mlp.gate_proj.g_idx', 'model.layers.0.mlp.gate_proj.g_idx', 'model.layers.1.mlp.down_proj.g_idx', 'model.layers.23.mlp.up_proj.g_idx', 'model.layers.28.self_attn.o_proj.g_idx', 'model.layers.5.mlp.up_proj.g_idx', 'model.layers.8.self_attn.q_proj.g_idx', 'model.layers.19.mlp.down_proj.g_idx', 'model.layers.14.mlp.down_proj.g_idx', 'model.layers.13.self_attn.v_proj.g_idx', 'model.layers.2.self_attn.k_proj.g_idx', 'model.layers.7.self_attn.o_proj.g_idx', 'model.layers.4.self_attn.o_proj.g_idx', 'model.layers.9.self_attn.q_proj.g_idx', 'model.layers.4.mlp.up_proj.g_idx', 'model.layers.28.mlp.gate_proj.g_idx', 'model.layers.19.mlp.up_proj.g_idx', 'model.layers.21.self_attn.q_proj.g_idx', 'model.layers.6.mlp.down_proj.g_idx', 'model.layers.2.self_attn.v_proj.g_idx', 'model.layers.24.mlp.up_proj.g_idx', 'model.layers.7.mlp.gate_proj.g_idx', 'model.layers.23.self_attn.o_proj.g_idx', 'model.layers.22.self_attn.o_proj.g_idx', 'model.layers.8.self_attn.v_proj.g_idx', 'model.layers.31.self_attn.o_proj.g_idx', 'model.layers.15.self_attn.q_proj.g_idx', 'model.layers.10.self_attn.q_proj.g_idx', 'model.layers.17.mlp.down_proj.g_idx', 'model.layers.19.self_attn.q_proj.g_idx', 'model.layers.6.self_attn.k_proj.g_idx', 'model.layers.5.self_attn.k_proj.g_idx', 'model.layers.12.mlp.down_proj.g_idx', 'model.layers.18.mlp.down_proj.g_idx', 'model.layers.18.self_attn.o_proj.g_idx', 'model.layers.21.self_attn.k_proj.g_idx', 'model.layers.12.self_attn.o_proj.g_idx', 'model.layers.10.self_attn.k_proj.g_idx', 'model.layers.18.self_attn.k_proj.g_idx', 'model.layers.23.mlp.gate_proj.g_idx', 'model.layers.3.mlp.down_proj.g_idx', 'model.layers.13.self_attn.o_proj.g_idx', 'model.layers.20.self_attn.q_proj.g_idx', 'model.layers.10.mlp.down_proj.g_idx', 'model.layers.6.self_attn.q_proj.g_idx', 'model.layers.14.mlp.gate_proj.g_idx', 'model.layers.31.mlp.down_proj.g_idx', 'model.layers.26.self_attn.o_proj.g_idx', 'model.layers.11.mlp.gate_proj.g_idx', 'model.layers.20.self_attn.o_proj.g_idx', 'model.layers.3.self_attn.o_proj.g_idx', 'model.layers.24.self_attn.q_proj.g_idx', 'model.layers.11.self_attn.o_proj.g_idx', 'model.layers.31.self_attn.v_proj.g_idx', 'model.layers.21.self_attn.o_proj.g_idx', 'model.layers.28.self_attn.k_proj.g_idx', 'model.layers.28.self_attn.v_proj.g_idx', 'model.layers.12.self_attn.k_proj.g_idx', 'model.layers.1.mlp.up_proj.g_idx', 'model.layers.15.self_attn.o_proj.g_idx', 'model.layers.29.self_attn.k_proj.g_idx', 'model.layers.0.self_attn.v_proj.g_idx', 'model.layers.7.self_attn.v_proj.g_idx', 'model.layers.15.self_attn.v_proj.g_idx', 'model.layers.17.mlp.up_proj.g_idx', 'model.layers.16.mlp.down_proj.g_idx', 'model.layers.12.mlp.gate_proj.g_idx', 'model.layers.14.self_attn.v_proj.g_idx', 'model.layers.10.mlp.gate_proj.g_idx', 'model.layers.30.mlp.down_proj.g_idx', 'model.layers.20.self_attn.k_proj.g_idx', 'model.layers.2.mlp.down_proj.g_idx', 'model.layers.2.self_attn.o_proj.g_idx', 'model.layers.30.mlp.up_proj.g_idx', 'model.layers.22.mlp.gate_proj.g_idx', 'model.layers.4.mlp.down_proj.g_idx', 'model.layers.4.self_attn.v_proj.g_idx', 'model.layers.5.self_attn.q_proj.g_idx', 'model.layers.13.mlp.down_proj.g_idx', 'model.layers.24.mlp.gate_proj.g_idx', 'model.layers.26.mlp.down_proj.g_idx', 'model.layers.16.self_attn.q_proj.g_idx', 'model.layers.3.self_attn.k_proj.g_idx', 'model.layers.5.self_attn.o_proj.g_idx', 'model.layers.3.self_attn.q_proj.g_idx', 'model.layers.6.self_attn.v_proj.g_idx', 'model.layers.29.self_attn.v_proj.g_idx', 'model.layers.20.mlp.down_proj.g_idx', 'model.layers.23.self_attn.q_proj.g_idx', 'model.layers.16.self_attn.k_proj.g_idx', 'model.layers.9.mlp.down_proj.g_idx', 'model.layers.27.self_attn.o_proj.g_idx', 'model.layers.25.mlp.down_proj.g_idx', 'model.layers.15.mlp.down_proj.g_idx', 'model.layers.17.self_attn.k_proj.g_idx', 'model.layers.17.self_attn.o_proj.g_idx', 'model.layers.21.mlp.down_proj.g_idx', 'model.layers.18.mlp.up_proj.g_idx', 'model.layers.20.mlp.gate_proj.g_idx', 'model.layers.16.mlp.gate_proj.g_idx', 'model.layers.27.mlp.down_proj.g_idx', 'model.layers.5.mlp.gate_proj.g_idx', 'model.layers.28.mlp.up_proj.g_idx', 'model.layers.31.self_attn.k_proj.g_idx', 'model.layers.2.mlp.up_proj.g_idx', 'model.layers.26.mlp.gate_proj.g_idx', 'model.layers.15.mlp.up_proj.g_idx', 'model.layers.27.mlp.gate_proj.g_idx', 'model.layers.2.self_attn.q_proj.g_idx', 'model.layers.0.mlp.down_proj.g_idx', 'model.layers.10.mlp.up_proj.g_idx', 'model.layers.24.self_attn.v_proj.g_idx', 'model.layers.2.mlp.gate_proj.g_idx', 'model.layers.14.self_attn.o_proj.g_idx', 'model.layers.21.mlp.up_proj.g_idx', 'model.layers.29.mlp.up_proj.g_idx', 'model.layers.26.self_attn.q_proj.g_idx', 'model.layers.30.self_attn.k_proj.g_idx', 'model.layers.9.self_attn.o_proj.g_idx', 'model.layers.7.mlp.up_proj.g_idx', 'model.layers.26.self_attn.k_proj.g_idx', 'model.layers.1.self_attn.o_proj.g_idx', 'model.layers.0.self_attn.q_proj.g_idx', 'model.layers.29.self_attn.o_proj.g_idx', 'model.layers.8.self_attn.o_proj.g_idx', 'model.layers.22.self_attn.k_proj.g_idx', 'model.layers.3.self_attn.v_proj.g_idx', 'model.layers.4.mlp.gate_proj.g_idx', 'model.layers.20.self_attn.v_proj.g_idx', 'model.layers.25.self_attn.k_proj.g_idx', 'model.layers.25.mlp.up_proj.g_idx', 'model.layers.14.self_attn.k_proj.g_idx', 'model.layers.27.self_attn.k_proj.g_idx', 'model.layers.13.mlp.gate_proj.g_idx', 'model.layers.14.self_attn.q_proj.g_idx', 'model.layers.7.self_attn.q_proj.g_idx', 'model.layers.9.mlp.gate_proj.g_idx', 'model.layers.24.self_attn.o_proj.g_idx', 'model.layers.29.self_attn.q_proj.g_idx', 'model.layers.22.self_attn.v_proj.g_idx', 'model.layers.31.self_attn.q_proj.g_idx', 'model.layers.17.mlp.gate_proj.g_idx', 'model.layers.3.mlp.up_proj.g_idx', 'model.layers.28.mlp.down_proj.g_idx', 'model.layers.21.self_attn.v_proj.g_idx', 'model.layers.29.mlp.down_proj.g_idx', 'model.layers.12.mlp.up_proj.g_idx', 'model.layers.30.mlp.gate_proj.g_idx', 'model.layers.25.self_attn.v_proj.g_idx', 'model.layers.10.self_attn.v_proj.g_idx', 'model.layers.27.self_attn.q_proj.g_idx', 'model.layers.26.self_attn.v_proj.g_idx', 'model.layers.9.mlp.up_proj.g_idx', 'model.layers.7.self_attn.k_proj.g_idx', 'model.layers.12.self_attn.q_proj.g_idx', 'model.layers.1.self_attn.k_proj.g_idx', 'model.layers.1.self_attn.q_proj.g_idx', 'model.layers.1.self_attn.v_proj.g_idx', 'model.layers.14.mlp.up_proj.g_idx', 'model.layers.8.mlp.down_proj.g_idx', 'model.layers.10.self_attn.o_proj.g_idx', 'model.layers.18.mlp.gate_proj.g_idx', 'model.layers.22.self_attn.q_proj.g_idx', 'model.layers.24.mlp.down_proj.g_idx', 'model.layers.11.mlp.down_proj.g_idx', 'model.layers.30.self_attn.o_proj.g_idx', 'model.layers.11.self_attn.k_proj.g_idx', 'model.layers.17.self_attn.q_proj.g_idx', 'model.layers.31.mlp.up_proj.g_idx', 'model.layers.8.self_attn.k_proj.g_idx', 'model.layers.19.self_attn.k_proj.g_idx', 'model.layers.25.self_attn.q_proj.g_idx', 'model.layers.9.self_attn.k_proj.g_idx', 'model.layers.13.self_attn.q_proj.g_idx', 'model.layers.30.self_attn.v_proj.g_idx', 'model.layers.5.self_attn.v_proj.g_idx', 'model.layers.0.self_attn.k_proj.g_idx', 'model.layers.13.mlp.up_proj.g_idx', 'model.layers.16.mlp.up_proj.g_idx', 'model.layers.20.mlp.up_proj.g_idx', 'model.layers.30.self_attn.q_proj.g_idx', 'model.layers.25.self_attn.o_proj.g_idx', 'model.layers.23.self_attn.v_proj.g_idx', 'model.layers.23.mlp.down_proj.g_idx', 'model.layers.16.self_attn.o_proj.g_idx', 'model.layers.31.mlp.gate_proj.g_idx', 'model.layers.24.self_attn.k_proj.g_idx', 'model.layers.18.self_attn.v_proj.g_idx', 'model.layers.8.mlp.up_proj.g_idx', 'model.layers.23.self_attn.k_proj.g_idx', 'model.layers.15.mlp.gate_proj.g_idx', 'model.layers.16.self_attn.v_proj.g_idx']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "base_model = \"TheBloke/WizardLM-7B-V1.0-Uncensored-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"oobaCUDA\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "\n",
    "\n",
    "#print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "#input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "#output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "#print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "def wizardlm_7b_inference(instruction):\n",
    "    prompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {instruction} ASSISTANT:\n",
    "    \n",
    "    '''\n",
    "    output = pipe(prompt_template)[0]['generated_text'].split(\"ASSISTANT:\")[1].strip()\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1b63315-9497-4133-b284-0fa458889795",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "#prompt = \"I need an 80s sci-fi fantasy character introduction for my newest novel. The main character’s name is Drake, and the year is 3098.\"\n",
    "#wizardlm_7b_inference(prompt)\n",
    "import json\n",
    "\n",
    "input_data_path = \"custom_data.jsonl\"\n",
    "output_data_path = \"WizardLM_7b_output.jsonl\"\n",
    "input_data = open(input_data_path, mode='r', encoding='utf-8')\n",
    "output_data = open(output_data_path, mode='w', encoding='utf-8')\n",
    "   \n",
    "for num, line in enumerate(input_data.readlines()):\n",
    "    print(num)\n",
    "    #print(line)\n",
    "    \n",
    "    one_data = json.loads(line)\n",
    "    id = one_data[\"id\"]\n",
    "    Category = one_data[\"type\"]\n",
    "    instruction = one_data[\"input\"]\n",
    "\n",
    "    final_output = wizardlm_7b_inference(instruction)\n",
    "       \n",
    "    new_data = {\n",
    "                \"id\": id,                   \n",
    "                \"instruction\": instruction,\n",
    "                \"wizardlm-13b\": final_output\n",
    "            }\n",
    "    output_data.write(json.dumps(new_data) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc63e3-066f-4404-b2c7-f3bea8647c08",
   "metadata": {},
   "source": [
    "## Vicuna 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f678a-7a0a-4afa-a953-8cd4aed67bd0",
   "metadata": {},
   "source": [
    "This code is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23400dae-771f-4985-963d-e1daee240bcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/vicuna-7B-v1.5-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "#prompt = \"Tell me about AI\"\n",
    "\n",
    "#print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "#input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "#output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "#print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "#print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "def vicuna_7b_inference(instruction):\n",
    "    prompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {instruction} ASSISTANT:\n",
    "\n",
    "'''\n",
    "    output = pipe(prompt_template)[0]['generated_text'].split(\"ASSISTANT:\")[1].strip()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d638b49b-3961-4c79-ab39-4809f55c7ec5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_data_path = \"custom_data.jsonl\"\n",
    "output_data_path = \"Vicuna_7b_output.jsonl\"\n",
    "input_data = open(input_data_path, mode='r', encoding='utf-8')\n",
    "output_data = open(output_data_path, mode='w', encoding='utf-8')\n",
    "   \n",
    "for num, line in enumerate(input_data.readlines()):\n",
    "    print(num)\n",
    "    #print(line)\n",
    "    \n",
    "    one_data = json.loads(line)\n",
    "    id = one_data[\"id\"]\n",
    "    Category = one_data[\"type\"]\n",
    "    instruction = one_data[\"input\"]\n",
    "\n",
    "    final_output = vicuna_7b_inference(instruction)\n",
    "       \n",
    "    new_data = {\n",
    "                \"id\": id,                   \n",
    "                \"instruction\": instruction,\n",
    "                \"vicuna-7b\": final_output\n",
    "            }\n",
    "    output_data.write(json.dumps(new_data) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4706172-b295-4b59-b0cb-ae65bafe5a37",
   "metadata": {},
   "source": [
    "## Tulu-2 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd1d664-376d-44c9-b2ae-e6173eb59bd3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "model_name_or_path = \"TheBloke/tulu-2-dpo-13B-AWQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "\n",
    "# Using the text streamer to stream output one token at a time\n",
    "#streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Convert prompt to tokens\n",
    "#tokens = tokenizer(\n",
    "#    prompt_template,\n",
    "#    return_tensors='pt'\n",
    "#).input_ids.cuda()\n",
    "\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "# Generate streamed output, visible one token at a time\n",
    "#generation_output = model.generate(\n",
    "#    tokens,\n",
    "#    streamer=streamer,\n",
    "#    **generation_params\n",
    "#)\n",
    "\n",
    "# Generation without a streamer, which will include the prompt in the output\n",
    "#generation_output = model.generate(\n",
    "#    tokens,\n",
    "#    **generation_params\n",
    "#)\n",
    "\n",
    "# Get the tokens from the output, decode them, print them\n",
    "#token_output = generation_output[0]\n",
    "#text_output = tokenizer.decode(token_output)\n",
    "#print(\"model.generate output: \", text_output)\n",
    "\n",
    "# Inference is also possible via Transformers' pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    **generation_params\n",
    ")\n",
    "\n",
    "#prompt = \"Tell me about AI\"\n",
    "\n",
    "def tulu_inference(prompt):\n",
    "    prompt_template=f'''<|user|>\n",
    "    {prompt}\n",
    "    <|assistant|>\n",
    "    '''\n",
    "    pipe_output = pipe(prompt_template)[0]['generated_text'].split(\"<|assistant|>\")[1].strip()\n",
    "    return pipe_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca2b305-e928-4b40-953d-350986d0f986",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_data_path = \"custom_data.jsonl\"\n",
    "output_data_path = \"tulu_13b_output.jsonl\"\n",
    "input_data = open(input_data_path, mode='r', encoding='utf-8')\n",
    "output_data = open(output_data_path, mode='w', encoding='utf-8')\n",
    "   \n",
    "for num, line in enumerate(input_data.readlines()):\n",
    "    print(num)\n",
    "    #print(line)\n",
    "    \n",
    "    one_data = json.loads(line)\n",
    "    id = one_data[\"id\"]\n",
    "    Category = one_data[\"type\"]\n",
    "    instruction = one_data[\"input\"]\n",
    "\n",
    "    final_output = tulu_inference(instruction)\n",
    "       \n",
    "    new_data = {\n",
    "                \"id\": id,                   \n",
    "                \"instruction\": instruction,\n",
    "                \"tulu-13b\": final_output\n",
    "            }\n",
    "    output_data.write(json.dumps(new_data) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c3d74-65ab-43cf-9069-7c6ff5ff4567",
   "metadata": {},
   "source": [
    "## Guanaco 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54aaa10b-89f3-422b-bddb-a0514535d717",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0213319781422f9814eadd5c607589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "adapters_name = 'timdettmers/guanaco-7b'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    #max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n",
    ")\n",
    "\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapters_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def guanaco_7b_inference(prompt):\n",
    "    #prompt = \"Introduce yourself\"\n",
    "    formatted_prompt = (\n",
    "        f\"A chat between a curious human and an artificial intelligence assistant.\"\n",
    "        f\"The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\"\n",
    "        f\"### Human: {prompt} ### Assistant:\"\n",
    "    )\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    outputs = model.generate(inputs=inputs.input_ids, **generation_params)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Assistant:\")[1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c89828-7438-4c93-b200-567f97206ff5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_data_path = \"custom_data.jsonl\"\n",
    "output_data_path = \"guanaco_7b_output.jsonl\"\n",
    "input_data = open(input_data_path, mode='r', encoding='utf-8')\n",
    "output_data = open(output_data_path, mode='w', encoding='utf-8')\n",
    "   \n",
    "for num, line in enumerate(input_data.readlines()):\n",
    "    print(num)\n",
    "    #print(line)\n",
    "    \n",
    "    one_data = json.loads(line)\n",
    "    id = one_data[\"id\"]\n",
    "    Category = one_data[\"type\"]\n",
    "    instruction = one_data[\"input\"]\n",
    "\n",
    "    final_output = guanaco_7b_inference(instruction)\n",
    "       \n",
    "    new_data = {\n",
    "                \"id\": id,                   \n",
    "                \"instruction\": instruction,\n",
    "                \"guanaco-7b\": final_output\n",
    "            }\n",
    "    output_data.write(json.dumps(new_data) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e5bca-846f-4b4c-985a-8619c74e7854",
   "metadata": {},
   "source": [
    "## Guanaco 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff9e74e5-97b7-4a86-bfd1-df5481f10281",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Guanaco-13B-Uncensored-GPTQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "def guanaco_13b_inference(prompt):\n",
    "    prompt_template=f'''### Human: {prompt}\n",
    "    ### Assistant:\n",
    "    \n",
    "    '''\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    \n",
    "    return pipe(prompt_template)[0]['generated_text'].split('Assistant')[1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea9afd9-0d3e-4cfd-a560-206c2180db17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_data_path = \"custom_data.jsonl\"\n",
    "output_data_path = \"guanaco_13b_output.jsonl\"\n",
    "input_data = open(input_data_path, mode='r', encoding='utf-8')\n",
    "output_data = open(output_data_path, mode='w', encoding='utf-8')\n",
    "   \n",
    "for num, line in enumerate(input_data.readlines()):\n",
    "    print(num)\n",
    "    #print(line)\n",
    "    \n",
    "    one_data = json.loads(line)\n",
    "    id = one_data[\"id\"]\n",
    "    Category = one_data[\"type\"]\n",
    "    instruction = one_data[\"input\"]\n",
    "\n",
    "    final_output = guanaco_13b_inference(instruction)\n",
    "       \n",
    "    new_data = {\n",
    "                \"id\": id,                   \n",
    "                \"instruction\": instruction,\n",
    "                \"guanaco-13b\": final_output\n",
    "            }\n",
    "    output_data.write(json.dumps(new_data) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9b63b-fee2-4f02-a6e4-3202501ea6c8",
   "metadata": {},
   "source": [
    "## Class to generate output from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f9bb18-4834-459f-876f-51de5cb454b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Union, Optional\n",
    "from transformers import pipeline, GenerationConfig\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from evaluate import load\n",
    "\n",
    "class ModelInference:\n",
    "    def __init__(self, model_list: List[str], dataset_name: str, model_index: int):\n",
    "        self.model_list = model_list\n",
    "        self.dataset_file = dataset_name\n",
    "        self.dataset = self.c_load_file('dataset')\n",
    "        self.model_output_file = None\n",
    "        self.file_type = None\n",
    "        self.model_index = model_index\n",
    "\n",
    "        \n",
    "    def load_model(self, model_name: str, task: str, max_new_tokens: int = 0) -> Any:\n",
    "        '''Create a Generation or Question answerer Pipeline with the given model'''\n",
    "        pipe = pipeline(task=task, model=model_name, device_map=\"cuda:0\")\n",
    "        if max_new_tokens > 0:\n",
    "            gen_cfg = GenerationConfig.from_model_config(pipe.config)\n",
    "            gen_cfg.max_new_tokens = max_new_tokens\n",
    "        return pipe\n",
    "\n",
    "    def c_load_file(self, file_type: str) -> Dict[Any, Any]:\n",
    "        self.file_type = file_type\n",
    "        file = {}\n",
    "        # Function to read file\n",
    "        def get_file_contents(filename, encoding='utf-8'):\n",
    "            with open(filename, encoding=encoding) as f:\n",
    "                content = f.read()\n",
    "            return content\n",
    "\n",
    "        # Function to read json file\n",
    "        def read_json(filename, encoding='utf-8'):\n",
    "            contents = get_file_contents(filename, encoding=encoding)\n",
    "            return json.loads(contents)\n",
    "\n",
    "        if self.file_type == 'dataset':\n",
    "            self.dataset = read_json(self.dataset_file)\n",
    "            file = self.dataset\n",
    "        elif self.file_type == 'model output':\n",
    "            self.model_output = read_json(self.model_output_file)\n",
    "            file = self.model_output\n",
    "        return file\n",
    "\n",
    "    def read_examples(self, no_of_examples: int = 1) -> List[Dict[str, Any]]:\n",
    "        examples = []\n",
    "        if self.file_type == None:\n",
    "            self.file_type = 'dataset'\n",
    "        if no_of_examples <= 0:\n",
    "            if self.file_type == 'dataset':\n",
    "                examples = self.dataset['examples']\n",
    "            elif self.file_type == 'model output':\n",
    "                examples = self.model_output['result']['examples']\n",
    "        else:\n",
    "            if self.file_type == 'dataset':\n",
    "                examples = self.dataset['examples'][:no_of_examples]\n",
    "            elif self.file_type == 'model output':\n",
    "                examples = self.model_output['result']['examples'][:no_of_examples]\n",
    "        return examples\n",
    "\n",
    "    def generate_output(self, example_dict: List[Dict[str, Any]], model: Any, max_new_tokens: int) -> List[Dict[str, Any]]:\n",
    "        output_list = []\n",
    "\n",
    "        for count, example in enumerate(tqdm(example_dict), start=1):\n",
    "            output_dict = {}\n",
    "            task = example['input']\n",
    "            if max_new_tokens > 0:\n",
    "                output = model(task, max_new_tokens=max_new_tokens, return_full_text=False)\n",
    "            else:\n",
    "                output = model(task, return_full_text=False)\n",
    "            output_dict['id'] = example['id']\n",
    "            output_dict['input'] = example['input']\n",
    "            output_dict['output'] = output[0]['generated_text']\n",
    "            output_dict['type'] = example['type']\n",
    "            output_list.append(output_dict)\n",
    "        return output_list\n",
    "\n",
    "    def save_result(self, output: List[Dict[str, Any]], model_name: str, model_index: str) -> None:\n",
    "        result_data = {\n",
    "            \"model_name\": model_name,\n",
    "            \"examples\": output\n",
    "        }\n",
    "\n",
    "        with open(f'result_model_{model_index}.json', 'w') as output_json_file:\n",
    "            json.dump({\"result\": result_data}, output_json_file)\n",
    "\n",
    "        print(f\"\\nResult Score saved to  result_model_{model_index}.json\")\n",
    "\n",
    "    def get_reference_data(self, no_of_examples) -> None:\n",
    "        examples = self.read_examples(no_of_examples)\n",
    "        \n",
    "        reference_data = {\"references\": []}\n",
    "    \n",
    "        # Iterate through each example\n",
    "        for example in examples:\n",
    "            reference_data[\"references\"].append({\"id\": example[\"id\"], \"input\": example[\"input\"], \"output\": example[\"target\"]})\n",
    "    \n",
    "        # Write the data in json file\n",
    "        with open(\"ref_data.json\", 'w') as reference_file:\n",
    "            json.dump(reference_data, reference_file)\n",
    "\n",
    "        self.reference_data = reference_data\n",
    "        print(f\"ref_data.json file created\")\n",
    "\n",
    "    # Common function to compute all scores\n",
    "    def compute_scores(self, no_of_examples: int = -1) -> 'ModelInference':\n",
    "        self.get_reference_data(no_of_examples) # get reference data\n",
    "        result_list = [] # List to store result\n",
    "        self.model_output_file = f'result_model_{self.model_index}.json'\n",
    "        file = self.c_load_file('model output')\n",
    "        example_list = self.read_examples(no_of_examples)\n",
    "        \n",
    "        for count, example in enumerate(tqdm(example_list), start=1):\n",
    "            score_dict = {} # dictionary to store scores\n",
    "            # get reference output\n",
    "            reference_output = [reference[\"output\"] for reference in self.reference_data[\"references\"] if reference[\"id\"] == example[\"id\"]]\n",
    "        \n",
    "            # Compute bleu score\n",
    "            google_bleu = load(\"google_bleu\")\n",
    "            google_bleu_result = google_bleu.compute(predictions=[example[\"output\"]], references=reference_output)\n",
    "            score_dict[\"google_bleu\"] = google_bleu_result[\"google_bleu\"]\n",
    "        \n",
    "            # Compute bert score\n",
    "            bertscore = load(\"bertscore\")\n",
    "            bertscore_result = bertscore.compute(predictions=[example[\"output\"]], references=reference_output, model_type='microsoft/deberta-xlarge-mnli') # using this because this is the recommended latest model by the library\n",
    "            score_dict[\"bertscore\"] = bertscore_result\n",
    "                \n",
    "            # Compute rouge score\n",
    "            rouge = load(\"rouge\")\n",
    "            rouge_results = rouge.compute(predictions=[example[\"output\"]], references=reference_output)\n",
    "            score_dict[\"rouge\"] = rouge_results\n",
    "            \n",
    "            # Add score dictionary to the result list\n",
    "            result_list.append({\"id\": example[\"id\"],\n",
    "                                \"input\": example[\"input\"], \n",
    "                                \"expected_output\": reference_output, \n",
    "                                \"generated_output\": example[\"output\"], \n",
    "                                \"scores\": score_dict})\n",
    "            self.score_list = result_list\n",
    "        return self\n",
    "\n",
    "    # Function to save scores\n",
    "    def save_scores(self):\n",
    "        score_data = {\n",
    "        \"model_name\" : self.model_list[self.model_index],\n",
    "        \"examples\" : self.score_list\n",
    "        }\n",
    "    \n",
    "        with open(f'score_model_{self.model_index}.json', 'w') as output_json_file:\n",
    "            json.dump({\"model_score\": score_data}, output_json_file)\n",
    "\n",
    "        print(f\"\\nScore saved to score_model_{self.model_index}.json\")\n",
    "\n",
    "    def run_inference(self, number_of_examples: int, max_new_tokens: int = 0) -> None:\n",
    "        model_name = self.model_list[self.model_index]\n",
    "        model = self.load_model(model_name, 'text-generation')\n",
    "        examples = self.read_examples(number_of_examples)\n",
    "        model_output = self.generate_output(examples, model, max_new_tokens)\n",
    "        self.save_result(model_output, model_name, str(self.model_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a010dde-5ff7-4b8a-b03f-02e0f598ff24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "model_list = ['TheBloke/Orca-2-13B-AWQ', 'microsoft/Orca-2-7b', '01-ai/Yi-6B', 'WizardLM/WizardLM-13B-V1.1',\n",
    "              'TheBloke/WizardLM-7B-V1.0-Uncensored-GPTQ', 'TheBloke/llava-v1.5-13B-AWQ', 'TheBloke/vicuna-7B-v1.5-GPTQ',\n",
    "              'TheBloke/tulu-2-dpo-13B-AWQ', 'timdettmers/guanaco-7b', 'TheBloke/guanaco-13B-GPTQ']\n",
    "\n",
    "model = ModelInference(model_list, dataset_name = 'custom_dataset.json', model_index=9)\n",
    "#model.run_inference(number_of_examples=1, max_new_tokens=-128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f061add-7206-4f79-8651-6611f310e10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_data.json file created\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b22561c81641f79d6145311b8f0af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score saved to score_model_9.json\n"
     ]
    }
   ],
   "source": [
    "model.compute_scores(no_of_examples=-1).save_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c656a-89ee-4670-8e7f-ece439672d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
